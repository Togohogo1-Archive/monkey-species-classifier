{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Importing Necessary Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-05T20:33:09.007804Z","iopub.status.busy":"2023-01-05T20:33:09.007403Z","iopub.status.idle":"2023-01-05T20:33:09.014088Z","shell.execute_reply":"2023-01-05T20:33:09.012512Z","shell.execute_reply.started":"2023-01-05T20:33:09.007770Z"},"trusted":true},"outputs":[],"source":["import math\n","import os\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision.io import read_image"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Setting up CUDA and TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-05T21:43:46.466468Z","iopub.status.busy":"2023-01-05T21:43:46.465861Z","iopub.status.idle":"2023-01-05T21:43:46.472885Z","shell.execute_reply":"2023-01-05T21:43:46.470847Z","shell.execute_reply.started":"2023-01-05T21:43:46.466435Z"},"trusted":true},"outputs":[],"source":["# CUDA\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","\n","# TensorBoard\n","writer = SummaryWriter()\n","print(f\"Writer: {writer}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Main Hyperparameters"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["image_base_size = 256\n","num_epochs = 2\n","batch_size = 10\n","learning_rate = 0.001"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Creating the Monkey Dataset and Loading in Training & Validation Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MonkeyDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform):\n","        self.annotations_file = annotations_file\n","        self.img_dir = img_dir\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.transform = transform\n","        self.number_samples = 0\n","\n","        # Adding all monkeys to a list for indexing\n","        self.monkeys = []\n","        for dirname, _, filenames in os.walk(self.img_dir):\n","            for filename in filenames:\n","                self.number_samples += 1\n","                self.monkeys.append((\n","                    os.path.join(dirname, filename),\n","                    filename\n","                ))\n","\n","    def __getitem__(self, index):\n","        monkey_path, monkey_filename = self.monkeys[index]\n","\n","        monkey = read_image(monkey_path)  # This already converts to a tensor\n","\n","        # Get monkey label from filename\n","        label = int(monkey_filename[1:2])  # Works since n0 to n9 all 2 characters\n","        \n","        # Apply the transforms\n","        if self.transform:\n","            tmonkey = self.transform(monkey)\n","        \n","        return tmonkey, label\n","\n","    def __len__(self):\n","        return self.number_samples\n","\n","    def get_label_map(self):\n","        # List of labels corresponding to monkey type\n","        return {i:j.strip() for i, j in zip(range(0, 10), self.img_labels.iloc[:, 2])}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Initializing Datasets and Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Composition of transforms\n","composed = transforms.Compose([\n","    transforms.ConvertImageDtype(torch.float32),\n","    transforms.Resize([image_base_size, image_base_size])  # Monkey images are not all square\n","])\n","\n","# Training and validation datasets (different from dataloaders)\n","# Datasets gets passed into a dataloader\n","training_data = MonkeyDataset(\n","    annotations_file = \"./kaggle/input/10-monkey-species/monkey_labels.txt\",\n","    img_dir = \"./kaggle/input/10-monkey-species/training/training\",\n","    transform = composed\n",")\n","\n","validation_data = MonkeyDataset(\n","    annotations_file = \"./kaggle/input/10-monkey-species/monkey_labels.txt\",\n","    img_dir = \"./kaggle/input/10-monkey-species/validation/validation\",\n","    transform = composed\n",")\n","\n","# Dataloaders\n","train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n","validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n","\n","# Peeking into the dataloader to get an example batch (train_dataloader is an iterator)\n","monkey_batch, train_labels = next(iter(train_dataloader))\n","\n","# Showing an example monkey\n","# https://stackoverflow.com/a/66641911\n","plt.imshow(monkey_batch[0].permute(1, 2, 0))\n","plt.show()\n","\n","# Use CUDA\n","monkey_batch = monkey_batch.to(device)\n","train_labels = train_labels.to(device)\n","\n","# [[[image_channel1, image_channel2, image_channel3], ...]], [label1, ...]\n","print(f\"Batch shape: {monkey_batch.shape}\")\n","print(f\"Image shape: {monkey_batch[0].shape}\")\n","\n","# Add batch of images to TensorBoard\n","writer.add_images(\"Example Batch\", monkey_batch)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Defining the Convolutional Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self) -> None:\n","        # Note: https://discuss.pytorch.org/t/dataset-inheritance-does-not-require-super/92945/2\n","        super().__init__()\n","\n","        # Classification learning\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n","        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3)\n","        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3)\n","        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n","        \n","        # Feature learning (regular NN part)\n","        self.full_con1 = nn.Linear(8*8*24, 1000)\n","        self.full_con2 = nn.Linear(1000, 100)\n","        self.full_con3 = nn.Linear(100, 10)\n","    \n","    def forward(self, x):\n","        # F.leaky_relu is a function, self.conv1 was from a class that called the function somewhere\n","        # Function takes in a tensor as input and outputs tensor\n","        s = self.pool(F.leaky_relu(self.conv1(x)))  \n","        s = self.pool(F.leaky_relu(self.conv2(s)))  \n","        s = self.pool(F.leaky_relu(self.conv3(s)))\n","\n","        # Output of classification learning squashed down:  channels * image width * image height\n","        s = torch.reshape(s, [-1, 8*8*24])\n","\n","        s = F.leaky_relu(self.full_con1(s))\n","        s = F.leaky_relu(self.full_con2(s))\n","        s = self.full_con3(s)  # The last activation function (softmax) is applied by loss function\n","\n","        return s\n","\n","# Use CUDA\n","model = CNN().to(device)\n","\n","# Add graph to TensorBoard\n","writer.add_graph(model, input_to_model=monkey_batch)\n","\n","print(model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Optimizing and Training the CNN"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10\n","tensor([8, 7, 1, 2, 8, 2, 9, 7, 8], device='cuda:0')\n","tensor([3, 6, 9, 9, 3, 9, 9, 9, 9], device='cuda:0') tensor([8, 7, 1, 2, 8, 2, 9, 7, 8], device='cuda:0')\n","Epoch:\t0\tIteration:\t0\tLoss:\t2.2788784503936768\n","10\n","tensor([3, 0, 0, 6, 8, 1, 8, 6, 5], device='cuda:0')\n","tensor([3, 6, 9, 9, 3, 9, 9, 9, 9], device='cuda:0') tensor([3, 0, 0, 6, 8, 1, 8, 6, 5], device='cuda:0')\n","Epoch:\t0\tIteration:\t1\tLoss:\t2.007495403289795\n","10\n","tensor([9, 2, 2, 7, 2, 8, 6, 8, 9], device='cuda:0')\n","tensor([3, 6, 9, 9, 3, 9, 9, 9, 9], device='cuda:0') tensor([9, 2, 2, 7, 2, 8, 6, 8, 9], device='cuda:0')\n","Epoch:\t0\tIteration:\t2\tLoss:\t2.0097546577453613\n","10\n","tensor([0, 6, 3, 3, 9, 7, 5, 2, 3], device='cuda:0')\n","tensor([3, 6, 9, 9, 3, 9, 9, 9, 9], device='cuda:0') tensor([0, 6, 3, 3, 9, 7, 5, 2, 3], device='cuda:0')\n","Epoch:\t0\tIteration:\t3\tLoss:\t2.164885997772217\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Training loop\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 8\u001b[0m     \u001b[39mfor\u001b[39;00m iteration, (_monkey_batch, _train_labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m      9\u001b[0m         \u001b[39m# Use CUDA\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         _monkey_batch \u001b[39m=\u001b[39m _monkey_batch\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m         _train_labels \u001b[39m=\u001b[39m _train_labels\u001b[39m.\u001b[39mto(device)\n","File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\LeEnv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\LeEnv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\LeEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n","File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\LeEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\LeEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\LeEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    139\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    140\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[0;32m    142\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m    143\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    145\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Loss and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","step = 0\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    for iteration, (_monkey_batch, _train_labels) in enumerate(train_dataloader):\n","        # Use CUDA\n","        _monkey_batch = _monkey_batch.to(device)\n","        _train_labels = _train_labels.to(device)\n","\n","        # Forward pass\n","        # Note: model.forward() is the same as model()\n","        pred = model.forward(_monkey_batch)  # Returns torch.Size([batch_size, 10])\n","        size_batch = pred.shape[1]\n","        print(size_batch)\n","\n","        # Clearing accumulated gradients from optimizer\n","        optimizer.zero_grad()\n","\n","        # Calculate loss and do backwards pass\n","        # Target is a value between 0 and C\n","        print(_train_labels)\n","        loss = loss_fn(pred, _train_labels)  # Notes for what gets passed here: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n","        loss.backward()  # Calculates gradients for optimizer step function\n","\n","        # Backpropagation\n","        optimizer.step()\n","\n","        _, max_pred = torch.max(pred, 1)\n","        # acc = \n","        print(a, _train_labels)\n","\n","        print(f\"Epoch:\\t{epoch}\\tIteration:\\t{iteration}\\tLoss:\\t{loss}\")\n","        writer.add_scalar(\"Loss/train\", loss, i)\n","        writer.flush()\n","\n","        step += 1\n","\n","writer.close()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Evaluating model accuracy with actual dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n","    tot = 0\n","    right = 0\n","    for monkey, label in validation_dataloader:  # Rename this to validation_dataloader later\n","        monkey = monkey.to(device)\n","        label = label.to(device)\n","        # print(monkey.size(), label) # Actual monkey, actual label\n","\n","        guess = model(monkey)\n","        maxes, indicies = torch.max(guess, 1)\n","        \n","        right += sum(indicies == label)\n","        tot += indicies.shape[0]\n","        # print(indicies, label, torch.sum())\n","\n","        # print(torch.sum)\n","\n","print(f\"Final Acc: {right/tot}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exporting Trained Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"WIP\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Random Testing Scripts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","e = pd.read_csv(\"./kaggle/input/10-monkey-species/monkey_labels.txt\")\n","e.iloc[:,3]\n","\n","tf = monkey_batch\n","# print(tf, type(tf), tf.size())\n","\n","conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n","pool = nn.MaxPool2d(kernel_size=3, stride=3)\n","conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3)\n","# self.pool2 = nn.MaxPool2d(kernel_size=3, stride=3)\n","conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3)\n","# self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","# End up with size of 30 for the regular neural network\n","full_con1 = nn.Linear(8*8*24, 100)\n","full_con2 = nn.Linear(100, 50)\n","full_con3 = nn.Linear(50, 10)\n","\n","# This whole first part is classification learning\n","s = pool(F.leaky_relu(conv1(tf)))  # Function takes in a tensor as input\n","print(type(s), s.size())\n","s = pool(F.leaky_relu(conv2(s)))\n","print(type(s), s.size())\n","s = pool(F.leaky_relu(conv3(s)))\n","print(type(s), s.size())\n","\n","# Output channels * image width * image height\n","# s.view(-1, 30*30*24).size()\n","s = torch.reshape(s, [-1, 8*8*24])\n","s.size()\n","# s = \n","\n","# This whole second part is feature learning\n","s = F.leaky_relu(full_con1(s))\n","print(type(s), s.size())\n","s = F.leaky_relu(full_con2(s))\n","print(type(s), s.size())\n","s = full_con3(s)\n","print(type(s), s.size())\n","\n","\n","print(model.base.parameters())\n","print(model.classifier.parameters())\n","for p in model.parameters():\n","    print(p)\n","'''"]}],"metadata":{"kernelspec":{"display_name":"LeEnv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"1e843574fdde30e771bbc8301b220735cbe0f09b2be0233465fc98bf8fc13769"}}},"nbformat":4,"nbformat_minor":4}
