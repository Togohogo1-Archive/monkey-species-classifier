{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Importing Necessary Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-05T20:33:09.007804Z","iopub.status.busy":"2023-01-05T20:33:09.007403Z","iopub.status.idle":"2023-01-05T20:33:09.014088Z","shell.execute_reply":"2023-01-05T20:33:09.012512Z","shell.execute_reply.started":"2023-01-05T20:33:09.007770Z"},"trusted":true},"outputs":[],"source":["import math\n","import os\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision.io import read_image"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Setting up CUDA and TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-05T21:43:46.466468Z","iopub.status.busy":"2023-01-05T21:43:46.465861Z","iopub.status.idle":"2023-01-05T21:43:46.472885Z","shell.execute_reply":"2023-01-05T21:43:46.470847Z","shell.execute_reply.started":"2023-01-05T21:43:46.466435Z"},"trusted":true},"outputs":[],"source":["# CUDA\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","\n","# TensorBoard\n","writer = SummaryWriter()\n","print(f\"Writer: {writer}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Main Hyperparameters"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["image_base_size = 256\n","num_epochs = 2\n","batch_size = 10\n","learning_rate = 0.001"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Creating the Monkey Dataset and Loading in Training & Validation Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MonkeyDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform):\n","        self.annotations_file = annotations_file\n","        self.img_dir = img_dir\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.transform = transform\n","        self.number_samples = 0\n","\n","        # Adding all monkeys to a list for indexing\n","        self.monkeys = []\n","        for dirname, _, filenames in os.walk(self.img_dir):\n","            for filename in filenames:\n","                self.number_samples += 1\n","                self.monkeys.append((\n","                    os.path.join(dirname, filename),\n","                    filename\n","                ))\n","\n","    def __getitem__(self, index):\n","        monkey_path, monkey_filename = self.monkeys[index]\n","\n","        monkey = read_image(monkey_path)  # This already converts to a tensor\n","\n","        # Get monkey label from filename\n","        label = int(monkey_filename[1:2])  # Works since n0 to n9 all 2 characters\n","        \n","        # Apply the transforms\n","        if self.transform:\n","            tmonkey = self.transform(monkey)\n","        \n","        return tmonkey, label\n","\n","    def __len__(self):\n","        return self.number_samples\n","\n","    def get_label_map(self):\n","        # List of labels corresponding to monkey type\n","        return {i:j.strip() for i, j in zip(range(0, 10), self.img_labels.iloc[:, 2])}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Initializing Datasets and Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Composition of transforms\n","composed = transforms.Compose([\n","    transforms.ConvertImageDtype(torch.float32),\n","    transforms.Resize([image_base_size, image_base_size])  # Monkey images are not all square\n","])\n","\n","# Training and validation datasets (different from dataloaders)\n","# Datasets gets passed into a dataloader\n","training_data = MonkeyDataset(\n","    annotations_file = \"./kaggle/input/10-monkey-species/monkey_labels.txt\",\n","    img_dir = \"./kaggle/input/10-monkey-species/training/training\",\n","    transform = composed\n",")\n","\n","validation_data = MonkeyDataset(\n","    annotations_file = \"./kaggle/input/10-monkey-species/monkey_labels.txt\",\n","    img_dir = \"./kaggle/input/10-monkey-species/validation/validation\",\n","    transform = composed\n",")\n","\n","# Dataloaders\n","train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n","validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n","\n","# Peeking into the dataloader to get an example batch (train_dataloader is an iterator)\n","monkey_batch, train_labels = next(iter(train_dataloader))\n","\n","# Showing an example monkey\n","# https://stackoverflow.com/a/66641911\n","plt.imshow(monkey_batch[0].permute(1, 2, 0))\n","plt.show()\n","\n","# Use CUDA\n","monkey_batch = monkey_batch.to(device)\n","train_labels = train_labels.to(device)\n","\n","# [[[image_channel1, image_channel2, image_channel3], ...]], [label1, ...]\n","print(f\"Batch shape: {monkey_batch.shape}\")\n","print(f\"Image shape: {monkey_batch[0].shape}\")\n","\n","# Add batch of images to TensorBoard\n","writer.add_images(\"Example Batch\", monkey_batch)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Defining the Convolutional Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self) -> None:\n","        # Note: https://discuss.pytorch.org/t/dataset-inheritance-does-not-require-super/92945/2\n","        super().__init__()\n","\n","        # Classification learning\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n","        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3)\n","        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3)\n","        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n","        \n","        # Feature learning (regular NN part)\n","        self.full_con1 = nn.Linear(8*8*24, 1000)\n","        self.full_con2 = nn.Linear(1000, 100)\n","        self.full_con3 = nn.Linear(100, 10)\n","    \n","    def forward(self, x):\n","        # F.leaky_relu is a function, self.conv1 was from a class that called the function somewhere\n","        # Function takes in a tensor as input and outputs tensor\n","        s = self.pool(F.leaky_relu(self.conv1(x)))  \n","        s = self.pool(F.leaky_relu(self.conv2(s)))  \n","        s = self.pool(F.leaky_relu(self.conv3(s)))\n","\n","        # Output of classification learning squashed down:  channels * image width * image height\n","        s = torch.reshape(s, [-1, 8*8*24])\n","\n","        s = F.leaky_relu(self.full_con1(s))\n","        s = F.leaky_relu(self.full_con2(s))\n","        s = self.full_con3(s)  # The last activation function (softmax) is applied by loss function\n","\n","        return s\n","\n","# Use CUDA\n","model = CNN().to(device)\n","\n","# Add graph to TensorBoard\n","writer.add_graph(model, input_to_model=monkey_batch)\n","\n","print(model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Optimizing and Training the CNN"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([9, 10])\n","tensor([7, 7, 1, 0, 3, 1, 2, 7, 1], device='cuda:0')\n","Epoch:\t0\tIteration:\t0\tLoss:\t2.2353527545928955\n","torch.Size([9, 10])\n","tensor([1, 3, 1, 9, 3, 8, 0, 0, 0], device='cuda:0')\n","Epoch:\t0\tIteration:\t1\tLoss:\t2.3572499752044678\n","torch.Size([9, 10])\n","tensor([4, 3, 7, 4, 1, 6, 1, 5, 5], device='cuda:0')\n","Epoch:\t0\tIteration:\t2\tLoss:\t2.2249977588653564\n","torch.Size([9, 10])\n","tensor([1, 8, 3, 6, 2, 2, 3, 1, 8], device='cuda:0')\n","Epoch:\t0\tIteration:\t3\tLoss:\t2.309932231903076\n","torch.Size([9, 10])\n","tensor([1, 2, 6, 7, 9, 6, 7, 7, 7], device='cuda:0')\n","Epoch:\t0\tIteration:\t4\tLoss:\t2.1779284477233887\n","torch.Size([9, 10])\n","tensor([6, 7, 9, 0, 4, 2, 3, 5, 1], device='cuda:0')\n","Epoch:\t0\tIteration:\t5\tLoss:\t2.3214821815490723\n","torch.Size([9, 10])\n","tensor([7, 3, 9, 7, 4, 9, 4, 3, 5], device='cuda:0')\n","Epoch:\t0\tIteration:\t6\tLoss:\t2.299919366836548\n","torch.Size([9, 10])\n","tensor([3, 8, 0, 0, 9, 0, 8, 5, 0], device='cuda:0')\n","Epoch:\t0\tIteration:\t7\tLoss:\t2.556520938873291\n","torch.Size([9, 10])\n","tensor([9, 2, 8, 6, 2, 6, 2, 9, 1], device='cuda:0')\n","Epoch:\t0\tIteration:\t8\tLoss:\t2.4020838737487793\n","torch.Size([9, 10])\n","tensor([8, 3, 9, 8, 6, 0, 1, 6, 8], device='cuda:0')\n","Epoch:\t0\tIteration:\t9\tLoss:\t2.340369939804077\n","torch.Size([9, 10])\n","tensor([5, 8, 5, 8, 5, 0, 0, 4, 6], device='cuda:0')\n","Epoch:\t0\tIteration:\t10\tLoss:\t2.392953395843506\n","torch.Size([9, 10])\n","tensor([7, 4, 3, 4, 6, 3, 1, 3, 6], device='cuda:0')\n","Epoch:\t0\tIteration:\t11\tLoss:\t2.2365870475769043\n","torch.Size([9, 10])\n","tensor([3, 1, 2, 6, 0, 8, 9, 2, 9], device='cuda:0')\n","Epoch:\t0\tIteration:\t12\tLoss:\t2.3107481002807617\n","torch.Size([9, 10])\n","tensor([9, 4, 0, 2, 3, 6, 1, 8, 5], device='cuda:0')\n","Epoch:\t0\tIteration:\t13\tLoss:\t2.294674873352051\n","torch.Size([9, 10])\n","tensor([4, 1, 0, 2, 0, 7, 0, 0, 5], device='cuda:0')\n","Epoch:\t0\tIteration:\t14\tLoss:\t2.274430751800537\n","torch.Size([9, 10])\n","tensor([4, 7, 5, 9, 3, 9, 0, 7, 3], device='cuda:0')\n","Epoch:\t0\tIteration:\t15\tLoss:\t2.302861213684082\n","torch.Size([9, 10])\n","tensor([2, 6, 4, 1, 5, 3, 7, 7, 4], device='cuda:0')\n","Epoch:\t0\tIteration:\t16\tLoss:\t2.299194574356079\n","torch.Size([9, 10])\n","tensor([6, 2, 4, 3, 0, 8, 5, 3, 3], device='cuda:0')\n","Epoch:\t0\tIteration:\t17\tLoss:\t2.2624876499176025\n","torch.Size([9, 10])\n","tensor([8, 2, 3, 4, 9, 1, 5, 2, 2], device='cuda:0')\n","Epoch:\t0\tIteration:\t18\tLoss:\t2.311565637588501\n","torch.Size([9, 10])\n","tensor([0, 7, 7, 0, 9, 7, 9, 4, 9], device='cuda:0')\n","Epoch:\t0\tIteration:\t19\tLoss:\t2.331191062927246\n","torch.Size([9, 10])\n","tensor([9, 9, 2, 0, 4, 1, 2, 2, 5], device='cuda:0')\n","Epoch:\t0\tIteration:\t20\tLoss:\t2.307539939880371\n","torch.Size([9, 10])\n","tensor([8, 6, 7, 6, 3, 6, 7, 2, 4], device='cuda:0')\n","Epoch:\t0\tIteration:\t21\tLoss:\t2.3216543197631836\n","torch.Size([9, 10])\n","tensor([2, 6, 4, 1, 7, 5, 3, 5, 3], device='cuda:0')\n","Epoch:\t0\tIteration:\t22\tLoss:\t2.26485538482666\n","torch.Size([9, 10])\n","tensor([7, 2, 5, 8, 6, 0, 5, 4, 5], device='cuda:0')\n","Epoch:\t0\tIteration:\t23\tLoss:\t2.2829713821411133\n","torch.Size([9, 10])\n","tensor([1, 0, 8, 6, 8, 6, 3, 7, 0], device='cuda:0')\n","Epoch:\t0\tIteration:\t24\tLoss:\t2.3319692611694336\n","torch.Size([9, 10])\n","tensor([6, 9, 5, 9, 8, 3, 1, 1, 3], device='cuda:0')\n","Epoch:\t0\tIteration:\t25\tLoss:\t2.3865513801574707\n","torch.Size([9, 10])\n","tensor([2, 9, 3, 5, 3, 0, 5, 3, 1], device='cuda:0')\n","Epoch:\t0\tIteration:\t26\tLoss:\t2.280139446258545\n","torch.Size([9, 10])\n","tensor([4, 2, 0, 5, 7, 8, 3, 2, 6], device='cuda:0')\n","Epoch:\t0\tIteration:\t27\tLoss:\t2.244576930999756\n","torch.Size([9, 10])\n","tensor([6, 1, 9, 6, 3, 5, 6, 7, 6], device='cuda:0')\n","Epoch:\t0\tIteration:\t28\tLoss:\t2.2995941638946533\n","torch.Size([9, 10])\n","tensor([0, 4, 3, 0, 3, 5, 0, 2, 8], device='cuda:0')\n","Epoch:\t0\tIteration:\t29\tLoss:\t2.217158555984497\n","torch.Size([9, 10])\n","tensor([6, 3, 6, 2, 2, 6, 6, 9, 7], device='cuda:0')\n","Epoch:\t0\tIteration:\t30\tLoss:\t2.230257749557495\n","torch.Size([9, 10])\n","tensor([6, 3, 8, 0, 0, 4, 1, 6, 0], device='cuda:0')\n","Epoch:\t0\tIteration:\t31\tLoss:\t2.21354079246521\n","torch.Size([9, 10])\n","tensor([3, 8, 9, 8, 6, 1, 4, 2, 6], device='cuda:0')\n","Epoch:\t0\tIteration:\t32\tLoss:\t2.285964250564575\n","torch.Size([9, 10])\n","tensor([2, 8, 7, 0, 6, 7, 0, 1, 3], device='cuda:0')\n","Epoch:\t0\tIteration:\t33\tLoss:\t2.194882869720459\n","torch.Size([9, 10])\n","tensor([9, 1, 2, 9, 6, 3, 8, 2, 6], device='cuda:0')\n","Epoch:\t0\tIteration:\t34\tLoss:\t2.2174296379089355\n","torch.Size([9, 10])\n","tensor([3, 0, 0, 9, 2, 5, 1, 3, 5], device='cuda:0')\n","Epoch:\t0\tIteration:\t35\tLoss:\t2.170032024383545\n","torch.Size([9, 10])\n","tensor([9, 4, 3, 7, 1, 1, 5, 5, 3], device='cuda:0')\n","Epoch:\t0\tIteration:\t36\tLoss:\t2.2589735984802246\n","torch.Size([9, 10])\n","tensor([6, 3, 7, 0, 2, 3, 8, 9, 5], device='cuda:0')\n","Epoch:\t0\tIteration:\t37\tLoss:\t2.125403881072998\n"]}],"source":["# Loss and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","i = 0\n","# Training loop\n","for epoch in range(num_epochs):\n","    for iteration, (_monkey_batch, _train_labels) in enumerate(train_dataloader):\n","        # Use CUDA\n","        _monkey_batch = _monkey_batch.to(device)\n","        _train_labels = _train_labels.to(device)\n","\n","        # Forward pass\n","        # Note: model.forward() is the same as model()\n","        pred = model.forward(_monkey_batch)  # Returns torch.Size([batch_size, 10])\n","\n","        # Clearing accumulated gradients from optimizer\n","        optimizer.zero_grad()\n","\n","        # Calculate loss and do backwards pass\n","        # Target is a value between 0 and C\n","        print(_train_labels)\n","        loss = loss_fn(pred, _train_labels)  # Notes for what gets passed here: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n","        loss.backward()  # Calculates gradients for optimizer step function\n","\n","        optimizer.step()\n","\n","        print(f\"Epoch:\\t{epoch}\\tIteration:\\t{iteration}\\tLoss:\\t{loss}\")\n","        writer.add_scalar(\"Loss/train\", loss, i)\n","        writer.flush()\n","        i += 1\n","\n","writer.close()\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Evaluating model accuracy with actual dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n","    tot = 0\n","    right = 0\n","    for monkey, label in validation_dataloader:  # Rename this to validation_dataloader later\n","        monkey = monkey.to(device)\n","        label = label.to(device)\n","        # print(monkey.size(), label) # Actual monkey, actual label\n","\n","        guess = model(monkey)\n","        maxes, indicies = torch.max(guess, 1)\n","        \n","        right += sum(indicies == label)\n","        tot += indicies.shape[0]\n","        # print(indicies, label, torch.sum())\n","\n","        # print(torch.sum)\n","\n","print(f\"Final Acc: {right/tot}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Random testing scrips down here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","e = pd.read_csv(\"./kaggle/input/10-monkey-species/monkey_labels.txt\")\n","e.iloc[:,3]\n","\n","tf = monkey_batch\n","# print(tf, type(tf), tf.size())\n","\n","conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n","pool = nn.MaxPool2d(kernel_size=3, stride=3)\n","conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3)\n","# self.pool2 = nn.MaxPool2d(kernel_size=3, stride=3)\n","conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3)\n","# self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","# End up with size of 30 for the regular neural network\n","full_con1 = nn.Linear(8*8*24, 100)\n","full_con2 = nn.Linear(100, 50)\n","full_con3 = nn.Linear(50, 10)\n","\n","# This whole first part is classification learning\n","s = pool(F.leaky_relu(conv1(tf)))  # Function takes in a tensor as input\n","print(type(s), s.size())\n","s = pool(F.leaky_relu(conv2(s)))\n","print(type(s), s.size())\n","s = pool(F.leaky_relu(conv3(s)))\n","print(type(s), s.size())\n","\n","# Output channels * image width * image height\n","# s.view(-1, 30*30*24).size()\n","s = torch.reshape(s, [-1, 8*8*24])\n","s.size()\n","# s = \n","\n","# This whole second part is feature learning\n","s = F.leaky_relu(full_con1(s))\n","print(type(s), s.size())\n","s = F.leaky_relu(full_con2(s))\n","print(type(s), s.size())\n","s = full_con3(s)\n","print(type(s), s.size())\n","\n","\n","print(model.base.parameters())\n","print(model.classifier.parameters())\n","for p in model.parameters():\n","    print(p)"]}],"metadata":{"kernelspec":{"display_name":"LeEnv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"1e843574fdde30e771bbc8301b220735cbe0f09b2be0233465fc98bf8fc13769"}}},"nbformat":4,"nbformat_minor":4}
